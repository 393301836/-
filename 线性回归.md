# 线性回归

标签（空格分隔）： 机器学习

---
> **基本形式**：给定d个属性的示例 $ x= (x_1 ; x_2 ; \cdots ; x_d) $，其中，$x_1$ 是 $x$在第个$i$属性上的取值。
$$f(x) = \omega_1x_1 + \omega_2x_2 + \cdots + \omega_nx_n + b $$
即 $f(x) = \omega^Tx + b$
其中 $\omega = (\omega_1;\omega_2;\cdots;\omega_d)$

 **1. 最简单的线性回归**
    给定数据集$D =\{ (x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m) \}$,其中$x_i = ({x_i}_1;{x_i}_2;\cdots;{x_i}_d)$, $y_i \in R$ 。目标是学得模型预测输出，算法思路如下：
$f(_i)=\omega x_i + b$ 使得$f(x_i)\simeq y_i$ 

$$(\omega^* ,b^* ) = \underset{(\omega, b)}{\arg\min}\sum_{i=1}^m (f(x_i)-y_i)^2 \\ = \underset{(\omega, b)}{\arg\min}\sum_{i=1}^m(y_i-\omega x_i - b)^2$$
&emsp;&emsp;**最小二乘法：**上式（西瓜书54 公式(3.4)） 基于最小化均方误差作为目标函数进行求解模型。
**解：**$E_{(\omega, b)}$ 分别对$\omega$ 和 $b$ 求偏导
$$\frac{\partial E_(\omega, b)}{\partial \omega} = \sum_{i=1}^m 2 (y_i - \omega x_i - b)(-x_i) \\
=2 \sum_{i=1}^m (\omega x_i^2 + (y_i - b)(-x_i))\\
=2(\omega \sum_{i=1}^m x_i^2 - \sum_{i=1}^m (y_i - b) x_i)$$

----------

$$\frac{\partial E_(\omega, b)}{\partial b} = \sum_{i=1}^m 2 (y_i - \omega x_i - b)(-1) \\
=2 \sum_{i=1}^m (b - (y_i - \omega x_i))\\
=2(mb - \sum_{i=1}^m (y_i - \omega x_i))$$

令上述两个偏导数为零可求得$\omega$ 和 $b$ 最优解的闭式解:
**首先,** $E_{(\omega, b)}$ 对 $b$ 的偏导为零，得
    $$b = \frac{1}{m}  \sum_{i=1}^m (y_i - \omega x_i)) $$
 **然后,** 将 $b$ 代入 $E_{(\omega, b)}$ 对 $\omega$ 的偏导再求解:
    $$\frac{\partial E_(\omega, b)}{\partial \omega} = 0 $$
    先展开 $\omega \sum_{i=1}^m x_i^2 - \sum_{i=1}^m x_i y_i + \sum_{i=1}^m b x_i = 0 $
    后代入b：
    $\omega \sum_{i=1}^m x_i^2 - \sum_{i=1}^m x_i y_i+\sum_{i=1}^m (\frac{1}{m} \sum_{i=1}^m(y_i - \omega x_i))x_i = 0 $ ;
   $\omega \sum_{i=1}^m x_i^2 - \sum_{i=1}^m x_i y_i+ \frac{1}{m} \sum_{i=1}^m x_i \sum_{i=1}^m(y_i - \omega x_i) = 0 $;
   $\omega \sum_{i=1}^m x_i^2 - \sum_{i=1}^m x_i y_i+ (\frac{1}{m} \sum_{i=1}^m x_i) \sum_{i=1}^m y_i - (\frac{1}{m} \sum_{i=1}^m x_i) \omega \sum_{i=1}^m  x_i = 0 $;
    $$\omega = \frac{\sum_{i=1}^m y_i (x_i - \bar x)}{\sum_{i=1}^m x_i^2 - \frac{1}{m} \sum_{i=1}^m x_i \sum_{i=1}^m x_i} $$
   &emsp; $$ = \frac{\sum_{i=1}^m y_i (x_i - \bar x)}{\sum_{i=1}^m x_i^2 - \frac{1}{m} (\sum_{i=1}^m x_i)^2 } $$
   
&emsp;&emsp;**参数估计：** 将目标函数分别对w 和 b 求偏导，然后令偏导数结果都为零（依据凸函数最优解的性质），可求出w和b 最优解的闭式解。
最后，代回原函数 $f(x) = \omega^Tx + b$ 得到最简单的线性模型。


----------

 **2. 多元线性回归**
  $f(x_i) = \omega^Tx_i + b$ 使  $f(x_i)\simeq y_i $
数据集D的参数矩阵：
$$
X =   \begin{pmatrix}
        {x_1}_1 & {x_1}_2 & \cdots & {x_1}_d & 1 \\
        {x_2}_1 & {x_2}_2 & \cdots & {x_2}_d & 1 \\
        \vdots & \vdots & \ddots & \vdots & \vdots \\
        {x_m}_1 & {x_m}_2 & \cdots & {x_m}_d & 1 \\
      \end{pmatrix} = 
      \begin{pmatrix}
        {x_1}^T  & 1 \\
        {x_2}^T  & 1 \\
        \vdots   & \vdots \\
        {x_m}^T  & 1 \\
      \end{pmatrix}
$$
参数向量：
$$    
\hat \omega =\begin{pmatrix}
        {x_1}^T  \\
        {x_2}^T  \\
        \vdots   \\
        {x_m}^T  \\
        b        \\
      \end{pmatrix}
$$
输出向量：
$$    
y = \begin{pmatrix}
        {y_1}  \\
        {y_2}  \\
        \vdots   \\
        {y_m}  \\
      \end{pmatrix}
$$
最小二乘法估算$\omega$ 和 $b$:
$$\hat \omega^* = \mathop{\arg\min}_{(w,b)} (y - X\hat \omega)^T(y - X\hat \omega)  $$
（即相当于最小化 $ ||y - X \hat \omega||^2 $）
令$E_{(\hat\omega)} = (y - X\hat \omega)^T(y - X\hat \omega) $ ，对$\hat \omega$求导得：
$$\frac{\partial E_\hat\omega }{\partial \hat \omega} = 2 X^T(X\hat\omega  - y)$$
上式根据[查表][1] 或者 这篇[blog][2] 可以计算出偏导，但对于矩阵和向量求导还是不够熟练。

  [1]: https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector_identities
  [2]: https://blog.csdn.net/wx_blue_pig/article/details/79791906
分类讨论：

 1. 如果$X^TX$ 为满秩矩阵或者正定矩阵时，令上式结果为零，得
    $$\hat\omega^* = (X^TX)^{-1}X^Ty$$
因 $\hat x_i = (x_i; 1) ; \hat\omega = (\omega ; b) $ ，所求的线性回归模型为：
$$ f(\hat x_i) = \hat x_i^T (X^TX)^{-1} X^T y $$
 2. 如果$X^TX$ 不是满秩矩阵时，这时主要由归纳偏好决定，常见的做法是引入正则化项（regularization）
$ \color {red}{(此处不懂正则化项是如何能够做到偏好归纳？懂的人 详细赐教)}$
...不再展开

